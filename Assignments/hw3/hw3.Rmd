---
title: "STATS 413 Hw3"
author: "Shu Zhou"
date: "2020/10/22"
output:  
    pdf_document:
    latex_engine: xelatex
    html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(ggplot2)
library(tidyr)
library(dplyr)
library(ggthemes)
library(corrplot)
library(RColorBrewer)
library(tidyverse)
library(GGally)
library(alr3)
library(MASS)
library(mvtnorm)
library(caret)
library(class)
library(ISLR)
```


This is the Assignment 3 of STATS 413
Author: Shu Zhou
UMID: 19342932


\section{Exercise 12}
__(a.)__
Assume $y = \beta_{a}x +\epsilon_{i}$ and $x = \beta_{b}y +\epsilon_{j}$, Hence, the OLS estimator
\begin{equation}
\hat{\beta_{a}} = \frac{\sum_{i}^{n}x_{i}y_{i}}{\sum x_{i}^2}
\end{equation}
\begin{equation}
\hat{\beta_{b}} = \frac{\sum_{i}^{n}x_{i}y_{i}}{\sum y_{i}^2}
\end{equation}
Hence, when the beta denominators are equal $\sum x_{i}^2=\sum y_{i}^2$, the coefficent of estimate are equal

__(b.)__
```{r}
set.seed(100)
x<-rnorm(100)
y<-5*x + rnorm(100)
lmX<-lm(y~x)
lmY<-lm(x~y)
summary(lmX)
summary(lmY)
```
It is obvious that $\hat{\beta_a} \ne \hat{\beta_b}$

__(c.)___
```{r}
set.seed(1)
x = rnorm(100, mean=1000, sd=0.1)
y = x
lmY <- lm(y ~ x)
lmX <- lm(x ~ y)
summary(lmY)
summary(lmX)
```
It is obvious that $\hat{\beta_a} = \hat{\beta_b} = 1$


\section{Exercise 14}
__(a.)__  
```{r}
set.seed (1)
x1=runif (100)
x2 =0.5* x1+rnorm (100) /10
y=2+2* x1 +0.3* x2+rnorm (100)
```
The form of the regression model is given by:
\begin{equation}
y = \beta_{0} + \beta_{1} x_1 + \beta_{2} x_2 + \varepsilon
\end{equation}
Where $\beta_{0}=2$, $\beta_{1}=2$ and $\beta_{2}=0.3$

__(b.)__
```{r}
cor(x1,x2)
plot(x1,x2, main = "Scatter plot of X2 v.s. X1")
```

__(c.)__
```{r}
model_3 <- lm(y~x1+x2)
summary(model_3)

```
\begin{itemize}
\item $\hat{\beta_{0}} =  2.1305$ $(\beta_{0} = 2)$
\item $\hat{\beta_{1}} =  1.4396$ $(\beta_{1} = 2)$
\item $\hat{\beta_{2}} =  1.0097$ $(\beta_{2} = 0.3)$
\end{itemize}

we can reject $H_0 : \beta_1=0$; but we cannot reject $H_0 : \beta_2=0$


__(d.)__
```{r}
model_4<- lm(y~x1)
summary(model_4)
```
We can reject $H_0 : \beta_1=0$


__(e.)__
```{r}
model_5<- lm(y~x2)
summary(model_4)
```
We can reject $H_0 : \beta_2=0$

__(f.)__
The results from (c.) to (e.) do not contradict each other.

Without the presence of other predictors, both $\beta_1$ and $\beta_2$ are statistically significant.
However, x2 does not provide sufficiently new information when fitting a model that already contains x1.
Hence,in the presence of other predictors, $\beta_2$ is no longer statistically significant.
__(g.)__
```{r}
x1=c(x1 , 0.1)
x2=c(x2 , 0.8)
y=c(y,6)
par(mfrow=c(2,2))
# regression with both x1 and x2
model_6 <- lm(y~x1+x2)
summary(model_6)
plot(model_6)
# regression with x1 only
model_7 <- lm(y~x2)
summary(model_7)
plot(model_7)
# regression with x2 only
model_8 <- lm(y~x1)
summary(model_8)
plot(model_8)
```


\begin{itemize}
\item In the regression with both x1 and x2, we can see that the new observation has the highest leverage and residual, which can be considered as an outlier.
\item  In the regression with x1, the new observation is still fairly high-leverage and have a large residual, so it can also be
considered and an outlier.
\item In the regression with x1, the new observation is still fairly high-leverage and have a large residual, so it can also be
considered and an outlier.
\end{itemize}
Hence, for this model, the new observation might not be considered influential, since in all cases it can be regarded as an outlier.


\section{Exercise 9.1 in ALR}

__(9.1.1)__
```{r}
Rpdata<-read.csv("Rpdata.csv")
pairs(Rpdata)
cor(Rpdata)
```

From the scatter plot and correlation, we can see tht the correlations between x1 and x2, x2 and x3, x3 and x4, x4 and x5,
x5 and x6 are all relatively high, which might cause colinearity.
__(9.1.2)__
```{r}
lm.model<-lm(y~x1+x2+x3+x4+x5+x6,data=Rpdata)
summary(lm.model)

```
There is nothing strange with the regression coefficients of variables, but the 
coefficient of intercept is non-significant.
__(9.1.3)__
```{r}
residualPlot(lm.model)
```
The plot says "Always plot residuals, you never know what you will find."
But the residuals itself is strange since it is has some relationship with the fitted values.

\section{Exercise 9.4 in ALR}
__(9.4.1)__
\begin{equation}
h_{ij} = x_{i}^{T}(X^{T}X)^{-1}x_{i} = \frac{1}{n} + \frac{(x_{i}-\bar{x})(x_{j}-\bar{x})}{SXX}
\end{equation}
Hence for the leverages $h_{ii}$
\begin{equation}
h_{ii} = \frac{1}{n} + \frac{(x_{i}-\bar{x})^2}{SXX}
\end{equation}

__(9.4.2)__
The cases with large leverage will have extreme $(x_{i}-\bar{x})^2$ values, hence the values on the extremely left or right side of the scatter plot will have high leverage values.
__(9.4.3)__
We simply let n = 1, hence in this case $x_{i} = \bar{x}$
Hence
\begin{equation}
h_{ii} = \frac{1}{n} + \frac{(x_{i}-\bar{x})^2}{SXX} = 1 + 0 = 1
\end{equation}

